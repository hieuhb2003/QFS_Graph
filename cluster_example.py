# -*- coding: utf-8 -*-
"""
File: grid_search_bertopic.py
M√¥ t·∫£: T·ª± ƒë·ªông h√≥a vi·ªác t√¨m ki·∫øm tham s·ªë t·ªët nh·∫•t cho BERTopic b·∫±ng Grid Search.
       Script s·∫Ω ch·∫°y nhi·ªÅu t·ªï h·ª£p tham s·ªë UMAP/HDBSCAN, ƒë√°nh gi√° t·ª´ng c√°i,
       v√† b√°o c√°o k·∫øt qu·∫£ t·ªïng h·ª£p c√πng v·ªõi b·ªô tham s·ªë t·ªët nh·∫•t.
"""

import json
import os
import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
import torch
# from umap import UMAP
from umap.umap_ import UMAP

from hdbscan import HDBSCAN
from sklearn import metrics
from sklearn.model_selection import ParameterGrid # <<< Th√™m import n√†y

def load_and_preprocess_data(data_path, model_name, max_tokens):
    """T·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ file."""
    print(f"\n[INFO] ƒêang t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu t·ª´: {data_path}...")
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
    except Exception as e:
        print(f"[L·ªñI] ƒê·ªçc file th·∫•t b·∫°i: {e}")
        return None
    
    docs = []
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    for item in all_data:
        if isinstance(item, str):
            sub_docs = item.split("|||||")
            for sub_doc in sub_docs:
                if not sub_doc.strip(): continue
                tokens = tokenizer.encode(sub_doc.strip(), add_special_tokens=False)
                if len(tokens) > max_tokens: tokens = tokens[:max_tokens]
                docs.append(tokenizer.decode(tokens))
    print(f"[INFO] T·ªïng s·ªë vƒÉn b·∫£n sau khi x·ª≠ l√Ω: {len(docs)}")
    return docs

def create_embeddings(docs, model_name, batch_size):
    """T·∫°o embedding song song tr√™n nhi·ªÅu GPU."""
    print(f"\n[INFO] ƒêang t·∫£i m√¥ h√¨nh embedding: {model_name}...")
    embedding_model = SentenceTransformer(model_name, device='cpu')
    
    gpu_count = torch.cuda.device_count()
    print(f"\n[INFO] B·∫Øt ƒë·∫ßu t·∫°o embeddings song song tr√™n {gpu_count} GPU...")
    pool = embedding_model.start_multi_process_pool()
    embeddings = embedding_model.encode_multi_process(docs, pool=pool, batch_size=batch_size)
    embedding_model.stop_multi_process_pool(pool)
    print(f"[SUCCESS] ƒê√£ t·∫°o xong embeddings v·ªõi shape: {embeddings.shape}")
    return embeddings

def evaluate_run(true_labels, predicted_labels):
    """H√†m ƒë√°nh gi√° v√† TR·∫¢ V·ªÄ m·ªôt dictionary ch·ª©a c√°c ƒëi·ªÉm s·ªë."""
    true_labels = np.array(true_labels)
    predicted_labels = np.array(predicted_labels)
    non_outlier_mask = predicted_labels != -1
    
    # N·∫øu t·∫•t c·∫£ l√† outlier, tr·∫£ v·ªÅ ƒëi·ªÉm s·ªë t·ªá nh·∫•t
    if np.sum(non_outlier_mask) == 0:
        return {"ARI": -1, "AMI": -1, "Homogeneity": -1, "Completeness": -1, "V-measure": -1, "Outlier %": 100.0}

    filtered_true = true_labels[non_outlier_mask]
    filtered_pred = predicted_labels[non_outlier_mask]
    
    scores = {
        "ARI": metrics.adjusted_rand_score(filtered_true, filtered_pred),
        "AMI": metrics.adjusted_mutual_info_score(filtered_true, filtered_pred),
        "Homogeneity": metrics.homogeneity_score(filtered_true, filtered_pred),
        "Completeness": metrics.completeness_score(filtered_true, filtered_pred),
        "V-measure": metrics.v_measure_score(filtered_true, filtered_pred),
        "Outlier %": 100 * (1 - (np.sum(non_outlier_mask) / len(predicted_labels)))
    }
    return scores

def main():
    # =============================================================================
    # B∆Ø·ªöC 1: C√ÅC THAM S·ªê C·∫§U H√åNH V√Ä L∆Ø·ªöI T√åM KI·∫æM
    # =============================================================================
    # --- C·∫•u h√¨nh tƒ©nh ---
    DATA_FILE_PATH = "/home/hungpv/projects/next_work/raw_data.json"
    GROUND_TRUTH_PATH = "/home/hungpv/projects/next_work/grouth_truth_cluster.json"
    EMBEDDING_MODEL_NAME = "BAAI/bge-m3"
    MAX_TOKENS_PER_DOC = 8192
    BATCH_SIZE = 16
    OUTPUT_DIR = "results_grid_search"
    PRIMARY_METRIC = "AMI" # Ch·ªâ s·ªë d√πng ƒë·ªÉ ch·ªçn model t·ªët nh·∫•t

    # --- ƒê·ªäNH NGHƒ®A L∆Ø·ªöI THAM S·ªê ƒê·ªÇ T√åM KI·∫æM ---
# --- ƒê·ªäNH NGHƒ®A L∆Ø·ªöI THAM S·ªê ƒê√É ƒê∆Ø·ª¢C TINH CH·ªàNH ---
    param_grid = {
        # ∆Øu ti√™n c√°c gi√° tr·ªã th·∫•p ƒë·ªÉ t·∫≠p trung v√†o c·∫•u tr√∫c local
        'UMAP_N_NEIGHBORS': [5, 10, 15], 
        
        # Gi·ªØ nguy√™n ƒë·ªÉ kh√°m ph√°
        'UMAP_N_COMPONENTS': [5, 10], 
        
        # Neo quanh con s·ªë b·∫°n bi·∫øt (5-10 docs)
        'HDBSCAN_MIN_CLUSTER_SIZE': [5, 8], 
        
        # Ch·ªâ d√πng c√°c gi√° tr·ªã r·∫•t th·∫•p ƒë·ªÉ t·ªëi ƒëa ƒë·ªô nh·∫°y
        'HDBSCAN_MIN_SAMPLES': [1, 2, 3] 
    }
    
    print("--- B·∫ÆT ƒê·∫¶U QUY TR√åNH GRID SEARCH CHO BERTOPIC ---")
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # =============================================================================
    # B∆Ø·ªöC 2: CHU·∫®N B·ªä D·ªÆ LI·ªÜU (CH·∫†Y 1 L·∫¶N)
    # =============================================================================
    docs = load_and_preprocess_data(DATA_FILE_PATH, EMBEDDING_MODEL_NAME, MAX_TOKENS_PER_DOC)
    embeddings = create_embeddings(docs, EMBEDDING_MODEL_NAME, BATCH_SIZE)
    
    # T·∫£i v√† x·ª≠ l√Ω ground truth ƒë·ªÉ ƒë√°nh gi√°
    with open(GROUND_TRUTH_PATH, 'r', encoding='utf-8') as f: true_clusters_list = json.load(f)
    doc_to_true_label = {}
    tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)
    for i, cluster_docs in enumerate(true_clusters_list):
        for doc in cluster_docs:
            tokens = tokenizer.encode(doc.strip(), add_special_tokens=False)
            if len(tokens) > MAX_TOKENS_PER_DOC: tokens = tokens[:MAX_TOKENS_PER_DOC]
            doc_to_true_label[tokenizer.decode(tokens).strip()] = f"True_Cluster_{i}"
    true_labels = [doc_to_true_label.get(doc.strip(), "N/A_Unlabeled") for doc in docs]

    # =============================================================================
    # B∆Ø·ªöC 3: V√íNG L·∫∂P GRID SEARCH
    # =============================================================================
    grid = ParameterGrid(param_grid)
    results = []
    best_score = -1
    best_params = None

    print(f"\n[INFO] B·∫Øt ƒë·∫ßu t√¨m ki·∫øm tr√™n {len(grid)} t·ªï h·ª£p tham s·ªë...")

    for i, params in enumerate(grid):
        print(f"\n--- Th·ª≠ nghi·ªám {i+1}/{len(grid)}: {params} ---")
        
        try:
            # Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn v·ªõi tham s·ªë hi·ªán t·∫°i
            umap_model = UMAP(n_neighbors=params['UMAP_N_NEIGHBORS'], n_components=params['UMAP_N_COMPONENTS'], min_dist=0.0, metric='cosine', random_state=42)
            hdbscan_model = HDBSCAN(min_cluster_size=params['HDBSCAN_MIN_CLUSTER_SIZE'], min_samples=params['HDBSCAN_MIN_SAMPLES'], metric='euclidean', cluster_selection_method='eom', prediction_data=True)
            
            # Kh·ªüi t·∫°o v√† hu·∫•n luy·ªán BERTopic
            topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=False)
            topics, _ = topic_model.fit_transform(docs, embeddings)

            # ƒê√°nh gi√° k·∫øt qu·∫£
            scores = evaluate_run(true_labels, topics)
            
            # L∆∞u k·∫øt qu·∫£
            current_run = {**params, **scores}
            results.append(current_run)
            
            # C·∫≠p nh·∫≠t k·∫øt qu·∫£ t·ªët nh·∫•t
            if scores[PRIMARY_METRIC] > best_score:
                best_score = scores[PRIMARY_METRIC]
                best_params = params
                print(f"üî•üî•üî• New best score found! {PRIMARY_METRIC}: {best_score:.4f}")

        except Exception as e:
            print(f"[L·ªñI] G·∫∑p l·ªói v·ªõi tham s·ªë {params}. L·ªói: {e}")
            results.append({**params, "ARI": "Error", "AMI": "Error"})

    # =============================================================================
    # B∆Ø·ªöC 4: B√ÅO C√ÅO K·∫æT QU·∫¢
    # =============================================================================
    print("\n\n--- HO√ÄN T·∫§T GRID SEARCH ---")
    
    # Chuy·ªÉn k·∫øt qu·∫£ sang DataFrame ƒë·ªÉ d·ªÖ xem
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values(by=PRIMARY_METRIC, ascending=False)
    
    # L∆∞u b·∫£ng t·ªïng h·ª£p k·∫øt qu·∫£
    results_df.to_csv(os.path.join(OUTPUT_DIR, "grid_search_summary.csv"), index=False, encoding='utf-8-sig')
    
    print(f"\nB·∫£ng t·ªïng h·ª£p k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {os.path.join(OUTPUT_DIR, 'grid_search_summary.csv')}")
    
    print("\n--- Top 5 c·∫•u h√¨nh t·ªët nh·∫•t ---")
    print(results_df.head(5))
    
    print("\n--- C·∫•u h√¨nh t·ªët nh·∫•t ƒë∆∞·ª£c t√¨m th·∫•y ---")
    print(f"ƒêi·ªÉm {PRIMARY_METRIC} cao nh·∫•t: {best_score:.4f}")
    print("V·ªõi c√°c tham s·ªë:")
    print(best_params)

if __name__ == "__main__":
    main()