#!/usr/bin/env python3
"""
Demo GraphRAG v·ªõi clustering integration
"""

import asyncio
import os
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from graphrag.clients.embedding_client import SentenceTransformersEmbeddingClient
from graphrag.clients.llm_client import OpenAIClient
from graphrag.core.graphrag_system import GraphRAGSystem


async def main():
    """Main demo function"""
    
    # Configuration
    working_dir = "graphrag_data"
    global_config = {
        "embedding_dimension": 768,
        "chunk_size": 1000,
        "max_workers": 4
    }
    
    # Initialize clients
    embedding_client = SentenceTransformersEmbeddingClient(
        model_name="BAAI/bge-m3",
        global_config=global_config
    )
    
    llm_client = OpenAIClient(
        model_name="gpt-3.5-turbo",
        api_key=os.getenv("OPENAI_API_KEY"),
        global_config=global_config
    )
    
    # Clustering configuration
    clustering_config = {
        "update_threshold": 5,  # Update model when 5 outliers collected
        "min_cluster_size": 3,
        "min_samples": 2,
        "similarity_threshold": 0.6,
        "model_name": "dynamic_bertopic_model"
    }
    
    # Initialize GraphRAG system with clustering
    graphrag = GraphRAGSystem(
        working_dir=working_dir,
        embedding_client=embedding_client,
        global_config=global_config,
        llm_client=llm_client,
        enable_clustering=True,
        clustering_config=clustering_config
    )
    
    print("üöÄ GraphRAG System with Clustering initialized!")
    
    # Sample documents for initial clustering training
    initial_docs = [
        "Python l√† m·ªôt ng√¥n ng·ªØ l·∫≠p tr√¨nh r·∫•t ph·ªï bi·∫øn hi·ªán nay.",
        "Th∆∞ vi·ªán Pandas gi√∫p x·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng b·∫£ng hi·ªáu qu·∫£.",
        "TensorFlow v√† PyTorch l√† hai framework h·ªçc s√¢u h√†ng ƒë·∫ßu.",
        "L·ªói tr√†n b·ªô nh·ªõ th∆∞·ªùng x·∫£y ra khi x·ª≠ l√Ω c√°c t·∫≠p d·ªØ li·ªáu l·ªõn.",
        "C·∫•u tr√∫c d·ªØ li·ªáu v√† gi·∫£i thu·∫≠t l√† n·ªÅn t·∫£ng c·ªßa khoa h·ªçc m√°y t√≠nh.",
        "Tr·∫≠n ƒë·∫•u b√≥ng ƒë√° gi·ªØa Vi·ªát Nam v√† Th√°i Lan di·ªÖn ra k·ªãch t√≠nh.",
        "Nhi·ªÅu c·∫ßu th·ªß ƒë√£ th·ªÉ hi·ªán phong ƒë·ªô xu·∫•t s·∫Øc trong gi·∫£i ƒë·∫•u n√†y.",
        "Hu·∫•n luy·ªán vi√™n tr∆∞·ªüng ƒë√£ c√≥ nh·ªØng ch·ªâ ƒë·∫°o chi·∫øn thu·∫≠t h·ª£p l√Ω.",
        "K·∫øt qu·∫£ c·ªßa m√¥n ƒëi·ªÅn kinh t·∫°i SEA Games th·∫≠t b·∫•t ng·ªù.",
        "L·ªãch thi ƒë·∫•u c·ªßa ƒë·ªôi tuy·ªÉn qu·ªëc gia ƒë√£ ƒë∆∞·ª£c c√¥ng b·ªë."
    ]
    
    # Initialize clustering model
    print("\nüìä Initializing clustering model...")
    clustering_success = await graphrag.initialize_clustering(initial_docs)
    if clustering_success:
        print("‚úÖ Clustering model initialized successfully!")
    else:
        print("‚ùå Failed to initialize clustering model")
    
    # Sample documents for processing (including outliers for clustering)
    documents = [
        # Technology documents (should cluster with existing tech docs)
        "M·ªôt l·∫≠p tr√¨nh vi√™n gi·ªèi c·∫ßn n·∫Øm v·ªØng c√°c thu·∫≠t to√°n c·ªët l√µi.",
        "L·ªói 'NoneType' object has no attribute '...' l√† l·ªói ph·ªï bi·∫øn trong Python.",
        "Machine learning algorithms require careful hyperparameter tuning.",
        
        # Sports documents (should cluster with existing sports docs)
        "B√†n th·∫Øng ·ªü ph√∫t cu·ªëi ƒë√£ ƒë·ªãnh ƒëo·∫°t tr·∫≠n ƒë·∫•u.",
        "C·∫ßu th·ªß xu·∫•t s·∫Øc nh·∫•t tr·∫≠n ƒë·∫•u ƒë√£ ƒë∆∞·ª£c b√¨nh ch·ªçn.",
        "ƒê·ªôi tuy·ªÉn qu·ªëc gia chu·∫©n b·ªã cho tr·∫≠n ƒë·∫•u quan tr·ªçng.",
        
        # Food documents (should be outliers, create new cluster)
        "C√¥ng th·ª©c l√†m m√≥n ph·ªü b√≤ H√† N·ªôi chu·∫©n v·ªã.",
        "C√°ch ch·ªçn m·ªôt tr√°i s·∫ßu ri√™ng ngon kh√¥ng ph·∫£i ai c≈©ng bi·∫øt.",
        "ƒê·ªÉ l√†m b√°nh m√¨, b·∫°n c·∫ßn chu·∫©n b·ªã b·ªôt v√† men n·ªü.",
        "Review nh√† h√†ng buffet l·∫©u n∆∞·ªõng m·ªõi m·ªü t·∫°i qu·∫≠n 1.",
        "C√°ch ∆∞·ªõp s∆∞·ªùn n∆∞·ªõng BBQ ngon nh∆∞ ngo√†i h√†ng.",
        "M√≥n b√∫n ch·∫£ l√† ƒë·∫∑c s·∫£n kh√¥ng th·ªÉ b·ªè qua khi ƒë·∫øn H√† N·ªôi.",
        
        # More technology documents
        "Deep learning models require large amounts of training data.",
        "Version control with Git is essential for collaborative development.",
        
        # More sports documents
        "Championship finals will be held next month.",
        "Team strategy focuses on defensive play.",
        
        # More food documents (should trigger cluster update)
        "Traditional Vietnamese cuisine includes many rice-based dishes.",
        "Street food culture is vibrant in Southeast Asian cities."
    ]
    
    print(f"\nüìÑ Processing {len(documents)} documents with clustering...")
    
    # Process documents in batch with clustering
    results = await graphrag.insert_documents_batch_with_llm(
        documents=documents,
        max_concurrent_docs=3
    )
    
    # Analyze results
    success_count = sum(1 for r in results if r.get("status") == "success")
    print(f"\n‚úÖ Successfully processed {success_count}/{len(documents)} documents")
    
    # Show clustering results
    print("\nüìä Clustering Results:")
    for result in results:
        if result.get("status") == "success":
            doc_id = result.get("doc_id", "unknown")
            cluster_result = result.get("cluster_result")
            if cluster_result:
                if cluster_result.is_outlier:
                    print(f"  üìÑ {doc_id[:20]}... -> OUTLIER (confidence: {cluster_result.confidence:.3f})")
                else:
                    print(f"  üìÑ {doc_id[:20]}... -> Cluster {cluster_result.cluster_id} (confidence: {cluster_result.confidence:.3f})")
    
    # Get clustering statistics
    print("\nüìà Clustering Statistics:")
    cluster_stats = await graphrag.get_clustering_statistics()
    if cluster_stats.get("clustering_enabled"):
        print(f"  Total documents processed: {cluster_stats.get('total_docs_processed', 0)}")
        print(f"  Outliers collected: {cluster_stats.get('outliers_collected', 0)}")
        print(f"  Model updates: {cluster_stats.get('model_updates', 0)}")
        print(f"  New clusters created: {cluster_stats.get('new_clusters_created', 0)}")
        print(f"  Total clusters: {cluster_stats.get('total_clusters', 0)}")
        print(f"  Outlier rate: {cluster_stats.get('outlier_rate', 0):.2%}")
        
        # Show cluster distribution
        distribution = cluster_stats.get("cluster_distribution", {})
        if distribution:
            print("  Cluster distribution:")
            for cluster_id, doc_count in distribution.items():
                print(f"    Cluster {cluster_id}: {doc_count} documents")
    
    # Demo clustering-aware queries
    print("\nüîç Clustering-Aware Queries:")
    
    queries = [
        "Python programming",
        "Vietnamese food",
        "football match",
        "machine learning algorithms"
    ]
    
    for query in queries:
        print(f"\n  Query: '{query}'")
        result = await graphrag.query_with_clustering(query, top_k=5, use_clusters=True)
        
        if "error" not in result:
            entities = result.get("entities", [])
            relations = result.get("relations", [])
            clustering_info = result.get("clustering_info")
            
            print(f"    Found {len(entities)} entities, {len(relations)} relations")
            
            if clustering_info and "similar_clusters" in clustering_info:
                similar_clusters = clustering_info["similar_clusters"]
                print(f"    Similar clusters: {len(similar_clusters)}")
                for cluster in similar_clusters[:2]:  # Show top 2
                    print(f"      Cluster {cluster['cluster_id']}: similarity={cluster['similarity']:.3f}, keywords={cluster['keywords'][:3]}")
        else:
            print(f"    Error: {result['error']}")
    
    # Demo getting documents by cluster
    print("\nüìã Documents by Cluster:")
    all_clusters = await graphrag.clustering_manager.get_all_clusters()
    for cluster in all_clusters[:3]:  # Show first 3 clusters
        cluster_id = cluster.cluster_id
        docs = await graphrag.get_documents_by_cluster(cluster_id)
        print(f"  Cluster {cluster_id} ({cluster.keywords[:3]}): {len(docs)} documents")
        for doc in docs[:2]:  # Show first 2 docs
            print(f"    - {doc['doc_id'][:20]}... (confidence: {doc.get('cluster_confidence', 0):.3f})")
    
    # Cleanup
    await graphrag.cleanup()
    print("\nüßπ Cleanup completed!")


if __name__ == "__main__":
    asyncio.run(main()) 